
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>第一章 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="1.html" />
    
    
    <link rel="prev" href="../" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.2" data-path="./">
            
                <a href="./">
            
                    
                    第一章
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="1.html">
            
                <a href="1.html">
            
                    
                    第一节 VMware16安装 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="2.html">
            
                <a href="2.html">
            
                    
                    第二节 Centos7 安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="3.html">
            
                <a href="3.html">
            
                    
                    第三节 Hadoop2.7 安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="4.html">
            
                <a href="4.html">
            
                    
                    第四节  Zookeeper安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="5.html">
            
                <a href="5.html">
            
                    
                    第五节  Hbase安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="6.html">
            
                <a href="6.html">
            
                    
                    第六节 主节点安装 MySql
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="7.html">
            
                <a href="7.html">
            
                    
                    第七节 主节点安装 Hive
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="8.html">
            
                <a href="8.html">
            
                    
                    第八节 flume安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="9.html">
            
                <a href="9.html">
            
                    
                    第九节 kafka安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="10.html">
            
                <a href="10.html">
            
                    
                    第十节 scala安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="11.html">
            
                <a href="11.html">
            
                    
                    第十一节spark安装
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >第一章</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="introduction">Introduction</h1>
<p>Apache-hadoop2.7.3
+
Spark2.0
&#x96C6;&#x7FA4;&#x642D;&#x5EFA;&#x6B65;&#x9AA4;
&#x4F59; &#x8F89;
&#x7248;&#x672C; V 1&#x76EE;&#x5F55;
&#x76EE;&#x5F55; ........................................................................................................................................... 2
&#x4E00;&#x3001; &#x73AF;&#x5883;&#x8BF4;&#x660E; .................................................................................................................... 4</p>
<ol>
<li>&#x786C;&#x4EF6;&#x8BBE;&#x5907;........................................................................................................................ 4</li>
<li>linux &#x7248;&#x672C; ....................................................................................................................... 4</li>
<li>JDK &#x7248;&#x672C; ......................................................................................................................... 4</li>
<li>&#x96C6;&#x7FA4;&#x8282;&#x70B9;........................................................................................................................ 4</li>
<li>HOST &#x914D;&#x7F6E; ...................................................................................................................... 4</li>
<li>&#x8F6F;&#x4EF6;&#x7248;&#x672C;........................................................................................................................ 4
&#x4E8C;&#x3001; &#x51C6;&#x5907;&#x5DE5;&#x4F5C; .................................................................................................................... 5
&#x4E09;&#x3001; &#x6279;&#x91CF;&#x542F;&#x52A8;&#x547D;&#x4EE4; ............................................................................................................ 5</li>
<li>&#x6279;&#x91CF;&#x5173;&#x673A;........................................................................................................................ 6</li>
<li>&#x6279;&#x91CF;&#x91CD;&#x542F;........................................................................................................................ 6</li>
<li>Hadoop &#x542F;&#x52A8;&#x548C;&#x5173;&#x95ED; ...................................................................................................... 6</li>
<li>&#x6279;&#x91CF; Zookeeper &#x542F;&#x52A8; .................................................................................................... 7</li>
<li>&#x6279;&#x91CF; Zookeeper &#x5173;&#x95ED; .................................................................................................... 7</li>
<li>hbase &#x542F;&#x52A8; ..................................................................................................................... 7</li>
<li>hive &#x542F;&#x52A8; ........................................................................................................................ 8
&#x56DB;&#x3001; zookeeper &#x5B89;&#x88C5; .......................................................................................................... 8
&#x4E94;&#x3001; &#x5B89;&#x88C5; Hadoop ............................................................................................................ 10
&#x516D;&#x3001; &#x4E3B;&#x8282;&#x70B9;&#x5B89;&#x88C5; hbase ................................................................................................... 18
&#x4E03;&#x3001; &#x4E3B;&#x8282;&#x70B9;&#x5B89;&#x88C5; MySql ................................................................................................... 20
&#x516B;&#x3001; &#x4E3B;&#x8282;&#x70B9;&#x5B89;&#x88C5; hive &#x548C;&#x542F;&#x52A8; ......................................................................................... 22
&#x4E5D;&#x3001; Apache &#x642D;&#x5EFA; Flume .................................................................................................. 24</li>
<li>Flume &#x4E0B;&#x8F7D;&#x5730;&#x5740;........................................................................................................... 24</li>
<li>&#x5B89;&#x88C5;.............................................................................................................................. 24</li>
<li>&#x914D;&#x7F6E;.............................................................................................................................. 25</li>
<li>&#x6DFB;&#x52A0; Java &#x8DEF;&#x5F84; ............................................................................................................. 25</li>
<li>&#x6D4B;&#x8BD5;&#x914D;&#x7F6E;...................................................................................................................... 26&#x5341;&#x3001; Kafka &#x5B89;&#x88C5;&#x548C;&#x4F7F;&#x7528; .................................................................................................... 26</li>
<li>&#x4E0B;&#x8F7D;&#x89E3;&#x538B;...................................................................................................................... 26</li>
<li>&#x5B89;&#x88C5;&#x914D;&#x7F6E;...................................................................................................................... 26</li>
<li>&#x542F;&#x52A8;.............................................................................................................................. 27</li>
<li>&#x6D4B;&#x8BD5;.............................................................................................................................. 28
&#x5341;&#x4E00;&#x3001; Scala &#x5B89;&#x88C5; ............................................................................................................. 29
&#x5341;&#x4E8C;&#x3001; Spakr &#x5B89;&#x88C5; ............................................................................................................ 30
&#x5341;&#x4E09;&#x3001; &#x542F;&#x52A8;&#x987A;&#x5E8F;&#x53CA;&#x8FDB;&#x7A0B;&#x89E3;&#x8BF4; .......................................................................................... 33
1) &#x8FDB;&#x7A0B;&#x89E3;&#x8BF4;...................................................................................................................... 33
2) &#x542F;&#x52A8;&#x987A;&#x5E8F;...................................................................................................................... 33
3) &#x5173;&#x95ED;&#x987A;&#x5E8F;...................................................................................................................... 33
4) &#x67E5;&#x770B;.............................................................................................................................. 34
5) hadoop &#x542F;&#x52A8;&#x6307;&#x4EE4; ........................................................................................................ 34
&#x5341;&#x56DB;&#x3001;
&#x9519;&#x8BEF;&#x96C6;&#x5408; .............................................................................................................. 34</li>
<li>&#x9519;&#x8BEF;:Mysql ................................................................................................................ 34</li>
<li>&#x9519;&#x8BEF;:Hbase ............................................................................................................... 35</li>
<li>&#x9519;&#x8BEF;:Hbase ............................................................................................................... 35</li>
<li>&#x9519;&#x8BEF;:hbase ................................................................................................................ 36</li>
<li>&#x9519;&#x8BEF;:Hbase &#x8FDE;&#x63A5;&#x96C6;&#x7FA4; ............................................................................................... 37</li>
<li>&#x9519;&#x8BEF;:HDFS &#x8FDE;&#x63A5;&#x96C6;&#x7FA4; ................................................................................................ 39</li>
<li>&#x9519;&#x8BEF;:NameNode ....................................................................................................... 41</li>
<li>&#x9519;&#x8BEF;:Hive01 .............................................................................................................. 42</li>
<li>&#x9519;&#x8BEF;:Hive02 .............................................................................................................. 43</li>
<li>&#x9519;&#x8BEF;:Hive03........................................................................................................... 43</li>
<li>&#x9519;&#x8BEF;:Hive04........................................................................................................... 43&#x4E00;&#x3001; &#x73AF;&#x5883;&#x8BF4;&#x660E;</li>
<li>&#x786C;&#x4EF6;&#x8BBE;&#x5907;
&#x4E00;&#x53F0;&#x7269;&#x7406;&#x673A;&#x9700;&#x8981;&#x5185;&#x5B58;&#x4E3A;&#x3010;16G&#x3011;</li>
<li>linux &#x7248;&#x672C;
[root@hadoop11 app]# cat /etc/issue
CentOS release 6.7 (Final)
Kernel \r on an \m</li>
<li>JDK &#x7248;&#x672C;
[root@hadoop11 app]# java -version
java version &quot;1.8.0_77&quot;
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)</li>
<li>&#x96C6;&#x7FA4;&#x8282;&#x70B9;
&#x4E09;&#x4E2A; hadoop11(Master),hadoop12(Slave),hadoop13(Slave)</li>
<li>HOST &#x914D;&#x7F6E;
192.168.200.11 hadoop11
192.168.200.12 hadoop12
192.168.200.13 hadoop13</li>
<li>&#x8F6F;&#x4EF6;&#x7248;&#x672C;jdk-8u77-linux-x64.tar.gz
zookeeper-3.4.8.tar.gz
hadoop-2.7.3.tar.gz
hbase-1.2.6-bin.tar
hive-0.12.0-bin.tar
apache-flume-1.6.0-bin.tar
kafka_2.10-0.8.1.1.tar.gz
&#x4E8C;&#x3001; &#x51C6;&#x5907;&#x5DE5;&#x4F5C;
1&#x3001;&#x5B89;&#x88C5; Java jdk
[root@hadoop11 app]# java -version
java version &quot;1.8.0_77&quot;
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)
2&#x3001;ssh &#x514D;&#x5BC6;&#x7801;&#x9A8C;&#x8BC1;
3&#x3001;&#x4E0B;&#x8F7D; Hadoop &#x7248;&#x672C;
4&#x3001;&#x6240;&#x6709;&#x8F6F;&#x4EF6;&#x653E;&#x5728; /app &#x76EE;&#x5F55;&#x4E0B;&#x9762;
[root@hadoop11 app]# ls
flume1.6
hadoop-2.7.3
hbase-1.2.6
zookeeper-3.4.8
&#x4E09;&#x3001; &#x6279;&#x91CF;&#x542F;&#x52A8;&#x547D;&#x4EE4;
hive-0.12.0
jdk1.8.0_77
kafka2.101. &#x6279;&#x91CF;&#x5173;&#x673A;
all_pc_halt.sh<h1 id="binsh">!/bin/sh</h1>
ssh root@hadoop11 &quot;bash&quot; &lt; /root/hadoop-halt.sh
ssh root@hadoop12 &quot;bash&quot; &lt; /root/hadoop-halt.sh
ssh root@hadoop13 &quot;bash&quot; &lt; /root/hadoop-halt.sh
pc-halt.sh<h1 id="binsh">!/bin/sh</h1>
halt</li>
<li>&#x6279;&#x91CF;&#x91CD;&#x542F;
all_pc_restart.sh<h1 id="binsh">!/bin/sh</h1>
ssh root@hadoop11 &quot;bash&quot; &lt; /root/hadoop-restart.sh
ssh root@hadoop12 &quot;bash&quot; &lt; /root/hadoop-restart.sh
ssh root@hadoop13 &quot;bash&quot; &lt; /root/hadoop-restart.sh
hadoop-restart.sh<h1 id="binsh">!/bin/sh</h1>
reboot</li>
<li>Hadoop &#x542F;&#x52A8;&#x548C;&#x5173;&#x95ED;
hadoop-start.sh<h1 id="binsh">!/bin/sh</h1>
sh /usr/app/hadoop-2.7.3/sbin/start-dfs.sh
sh /usr/app/hadoop-2.7.3/sbin/start-yarn.sh
hadoop-stop.sh<h1 id="binsh">!/bin/sh</h1>
sh /usr/app/hadoop-2.7.3/sbin/stop-dfs.sh
sh /usr/app/hadoop-2.7.3/sbin/stop-yarn.sh4. &#x6279;&#x91CF; Zookeeper &#x542F;&#x52A8;
all-zookeeper-start.sh<h1 id="binsh">!/bin/sh</h1>
ssh root@hadoop11 &quot;bash&quot; &lt; /root/zookeeper-start.sh
ssh root@hadoop12 &quot;bash&quot; &lt; /root/zookeeper-start.sh
ssh root@hadoop13 &quot;bash&quot; &lt; /root/zookeeper-start.sh
zookeeper-start.sh<h1 id="binsh">!/bin/sh</h1>
/usr/app/zookeeper-3.4.8/bin/./zkServer.sh start</li>
<li>&#x6279;&#x91CF; Zookeeper &#x5173;&#x95ED;
all-zookeeper-stop.sh<h1 id="binsh">!/bin/sh</h1>
ssh root@hadoop11 &quot;bash&quot; &lt; /root/zookeeper-stop.sh
ssh root@hadoop12 &quot;bash&quot; &lt; /root/zookeeper-stop.sh
ssh root@hadoop13 &quot;bash&quot; &lt; /root/zookeeper-stop.sh
zookeeper-stop.sh<h1 id="binsh">!/bin/sh</h1>
/usr/app/zookeeper-3.4.8/bin/./zkServer.sh stop</li>
<li>hbase &#x542F;&#x52A8;
hbase-start.sh<h1 id="binsh">!/bin/sh</h1>
sh /usr/app/hbase-1.2.6/bin/start-hbase.sh7. hive &#x542F;&#x52A8;
hive-start.sh<h1 id="binsh">!/bin/sh</h1>
sh /usr/app/hive-0.12.0/bin/hive
&#x56DB;&#x3001; zookeeper &#x5B89;&#x88C5;
1.&#x4E0A;&#x4F20; zk &#x5B89;&#x88C5;&#x5305;
[root@hadoop11 app]# ls
hadoop-2.7.3.tar.gz jdk1.8.0_77 zookeeper-3.4.8.tar.gz
2.&#x89E3;&#x538B;
tar -zxvf zookeeper-3.4.8.tar.gz
-C /usr/app/
3.&#x914D;&#x7F6E;(&#x5148;&#x5728;&#x4E00;&#x53F0;&#x8282;&#x70B9;&#x4E0A;&#x914D;&#x7F6E;)
3.1 &#x6DFB;&#x52A0;&#x4E00;&#x4E2A; zoo.cfg &#x914D;&#x7F6E;&#x6587;&#x4EF6;
cd zookeeper-3.4.8/conf/
cp -r zoo_sample.cfg zoo.cfg
3.2 &#x4FEE;&#x6539;&#x914D;&#x7F6E;&#x6587;&#x4EF6;(zoo.cfg)
&#x5EFA;&#x7ACB;/usr/app/zookeeper-3.4.8/data &#x76EE;&#x5F55;,
mkdir /usr/app/zookeeper-3.4.8/data
&#x914D;&#x7F6E; zoo.cfg
dataDir=/usr/app/zookeeper-3.4.8/data (the directory where the snapshot is stored.)
&#x5728;&#x6700;&#x540E;&#x4E00;&#x884C;&#x6DFB;&#x52A0;
server.1=hadoop11:2888:3888
server.2=hadoop12:2888:3888
server.3=hadoop13:2888:3888
3.3 &#x5728;(dataDir=/usr/app/zookeeper-3.4.8/data)&#x521B;&#x5EFA;&#x4E00;&#x4E2A; myid &#x6587;&#x4EF6;,&#x91CC;&#x9762;&#x5185;&#x5BB9;&#x662F; server.N &#x4E2D;
&#x7684; N(server.2 &#x91CC;&#x9762;&#x5185;&#x5BB9;&#x4E3A; 2)
echo &quot;1&quot; &gt;myid3.4 &#x5C06;&#x914D;&#x7F6E;&#x597D;&#x7684; zk &#x62F7;&#x8D1D;&#x5230;&#x5176;&#x4ED6;&#x8282;&#x70B9;
scp -r /usr/app/zookeeper-3.4.8/
scp -r /usr/app/zookeeper-3.4.8/
root@hadoop12:/usr/app
root@hadoop13:/usr/app
3.5 &#x6CE8;&#x610F;:&#x5728;&#x5176;&#x4ED6;&#x8282;&#x70B9;&#x4E0A;&#x4E00;&#x5B9A;&#x8981;&#x4FEE;&#x6539; myid &#x7684;&#x5185;&#x5BB9;
&#x5728; hadoop12 &#x5E94;&#x8BE5;&#x8BB2; myid &#x7684;&#x5185;&#x5BB9;&#x6539;&#x4E3A; 2 (echo &quot;2&quot; &gt;myid)
&#x5728; hadoop13 &#x5E94;&#x8BE5;&#x8BB2; myid &#x7684;&#x5185;&#x5BB9;&#x6539;&#x4E3A; 3 (echo &quot;3&quot; &gt;myid)
4.&#x542F;&#x52A8;&#x96C6;&#x7FA4;
&#x5206;&#x522B;&#x6BCF;&#x53F0;&#x8282;&#x70B9;&#x4E0A;&#x9762;&#x7684; Zookeeper,&#x542F;&#x52A8;&#x547D;&#x4EE4;:
/usr/app/zookeeper-3.4.8/bin/./zkServer.sh
start
&#x9009;&#x51FA; leader &#x548C; follower,Zookeeper &#x542F;&#x52A8;&#x7684;&#x5173;&#x95ED;&#x547D;&#x4EE4;
/usr/app/zookeeper-3.4.8/bin/./zkServer.sh
/usr/app/zookeeper-3.4.8/bin/./zkServer.sh
start
stop
5.&#x67E5;&#x770B;&#x542F;&#x52A8;&#x72B6;&#x6001;&#x67E5;&#x770B;&#x547D;&#x4EE4;
/usr/app/zookeeper-3.4.8/bin/./zkServer.sh status
6&#x3001;Zookeeper &#x64CD;&#x4F5C;
&#x8282;&#x70B9;&#x4E2D;&#x6570;&#x636E;&#x662F;&#x540C;&#x6B65;&#x7684;
&#x5B98;&#x7F51; <a href="http://zookeeper.apache.org/doc/r3.3.3/api/org/apache/zookeeper/ZooKeeper.html" target="_blank">http://zookeeper.apache.org/doc/r3.3.3/api/org/apache/zookeeper/ZooKeeper.html</a>
&#x53C2;&#x8003;:<a href="http://blog.csdn.net/ganglia/article/details/11606807&#x542F;&#x52A8;&#x547D;&#x4EE4;" target="_blank">http://blog.csdn.net/ganglia/article/details/11606807&#x542F;&#x52A8;&#x547D;&#x4EE4;</a> :
bash zkCli.sh -server localhost:2181
[zk: localhost:2181(CONNECTED) 7] ls /
[zookeeper]
&#x547D;&#x4EE4;
create
ls
get
set
rmr
&#x8DEF;&#x5F84;
&#x6570;&#x636E;
/hadoop &quot;myData&quot;
/
/hadoop
/hadoop &quot;11&quot;
/hadoop
&#x4E94;&#x3001; &#x5B89;&#x88C5; Hadoop
&#x8FD9;&#x662F;&#x4E0B;&#x8F7D;&#x540E;&#x7684; hadoop-2.7.3.tar.gz &#x538B;&#x7F29;&#x5305;,
1&#x3001;&#x89E3;&#x538B; tar -xzvf hadoop-2.7.3.tar.gz
2&#x3001;&#x914D;&#x7F6E;&#x73AF;&#x5883;&#x53D8;&#x91CF; vi /etc/profile
export JAVA_HOME=/usr/app/jdk1.8.0_77export HADOOP_HOME=/usr/app/hadoop-2.7.3
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin
3&#x3001;&#x4E3B;&#x8981;&#x6587;&#x4EF6;&#x914D;&#x7F6E;
3.1&#x3001;&#x914D;&#x7F6E; hadoop-env.sh &#x6587;&#x4EF6;--&gt;&#x4FEE;&#x6539; JAVA_HOME
/usr/app/hadoop-2.7.3/etc/hadoop<h1 id="the-java-implementation-to-use">The java implementation to use.</h1>
export JAVA_HOME=/usr/app/jdk1.8.0_77
3.2&#x3001;&#x914D;&#x7F6E; slaves &#x6587;&#x4EF6;--&gt;&gt;&#x589E;&#x52A0; slave &#x8282;&#x70B9;
/usr/app/hadoop-2.7.3/etc/hadoop
hadoop11
hadoop12
hadoop13
3.3 &#x3001; &#x914D; &#x7F6E; core-site.xml &#x6587; &#x4EF6; --&gt;&gt; &#x589E; &#x52A0; hadoop &#x6838; &#x5FC3; &#x914D; &#x7F6E; ( hdfs &#x6587; &#x4EF6; &#x7AEF; &#x53E3; &#x662F; 9000 &#x3001;
file:/usr/app/hadoop-2.7.3/tmp)
<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://ns1</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/usr/app/hadoop-2.7.3/tmp</value>
</property>
<property>
<name>ha.zookeeper.quorum</name>
<value>hadoop11:2181,hadoop12:2181,hadoop13:2181</value>
</property>
</configuration>
3.4&#x3001;&#x914D;&#x7F6E; hdfs-site.xml &#x6587;&#x4EF6;--&gt;&gt;&#x589E;&#x52A0; hdfs &#x914D;&#x7F6E;&#x4FE1;&#x606F;(namenode&#x3001; datanode &#x7AEF;&#x53E3;&#x548C;&#x76EE;&#x5F55;&#x4F4D;&#x7F6E;)
<configuration>
<property>
<name>dfs.nameservices</name>
<value>ns1</value>
</property>
<property>
<name>dfs.ha.namenodes.ns1</name>
<value>nn1,nn2</value>
</property><property>
<name>dfs.namenode.rpc-address.ns1.nn1</name>
<value>hadoop11:9000</value>
</property>
<property>
<name>dfs.namenode.http-address.ns1.nn1</name>
<value>hadoop11:50070</value>
</property>
<property>
<name>dfs.namenode.rpc-address.ns1.nn2</name>
<value>hadoop12:9000</value>
</property>
<property>
<name>dfs.namenode.http-address.ns1.nn2</name>
<value>hadoop12:50070</value>
</property>
<property>
<name>dfs.namenode.shared.edits.dir</name>
<value>qjournal://hadoop11:8485;hadoop12:8485;hadoop13:8485/ns1</value>
</property>
<property>
<name>dfs.journalnode.edits.dir</name>
<value>/usr/app/hadoop-2.7.3/journal/data</value>
</property>
<property>
<name>dfs.ha.automatic-failover.enabled</name>
<value>true</value>
</property>
<property>
<name>dfs.client.failover.proxy.provider.ns1</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<property>
<name>dfs.ha.fencing.methods</name>
<value>
sshfence
shell(/bin/true)
</value>
</property>
<property>
<name>dfs.ha.fencing.ssh.private-key-files</name>
<value>/root/.ssh/id_rsa</value>
</property>
<property><name>dfs.ha.fencing.ssh.connect-timeout</name>
<value>30000</value>
</property>
</configuration>
3.5&#x3001;&#x914D;&#x7F6E; mapred-site.xml &#x6587;&#x4EF6;--&gt;&gt;&#x589E;&#x52A0; mapreduce &#x914D;&#x7F6E;(&#x4F7F;&#x7528; yarn &#x6846;&#x67B6;&#x3001;jobhistory &#x4F7F;&#x7528;
&#x5730;&#x5740;&#x4EE5;&#x53CA; web &#x5730;&#x5740;)
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>
3.6&#x3001;&#x914D;&#x7F6E; yarn-site.xml &#x6587;&#x4EF6;--&gt;&gt;&#x589E;&#x52A0; yarn &#x529F;&#x80FD;
<configuration>
<property>
<name>yarn.resourcemanager.ha.enabled</name>
<value>true</value>
</property>
<property>
<name>yarn.resourcemanager.cluster-id</name>
<value>yrc</value>
</property>
<property>
<name>yarn.resourcemanager.ha.rm-ids</name>
<value>rm1,rm2</value>
</property>
<property>
<name>yarn.resourcemanager.hostname.rm1</name>
<value>hadoop11</value>
</property>
<property>
<name>yarn.resourcemanager.hostname.rm2</name>
<value>hadoop12</value>
</property>
<property>
<name>yarn.resourcemanager.zk-address</name>
<value>hadoop11:2181,hadoop12:2181,hadoop13:2181</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name><value>mapreduce_shuffle</value>
</property>
</configuration>
4&#x3001;&#x5C06;&#x914D;&#x7F6E;&#x597D;&#x7684; hadoop &#x6587;&#x4EF6; copy &#x5230;&#x53E6;&#x4E00;&#x53F0; slave &#x673A;&#x5668;&#x4E0A;
[root@hadoop11 app]# scp -r hadoop-2.7.3 root@hadoop13:/usr/app
5 &#x542F;&#x52A8; journalnode(&#x5206;&#x522B;&#x5728;&#x5728; hadoop11&#x3001;hadoop12&#x3001;hadoop13 &#x4E0A;&#x6267;&#x884C;)
cd /usr/app/hadoop-2.7.3
sbin/hadoop-daemon.sh start journalnode<h1 id="&#x8FD0;&#x884C;-jps-&#x547D;&#x4EE4;&#x68C0;&#x9A8C;hadoop11&#x3001;hadoop12&#x3001;hadoop13-&#x4E0A;&#x591A;&#x4E86;-journalnode-&#x8FDB;&#x7A0B;">&#x8FD0;&#x884C; jps &#x547D;&#x4EE4;&#x68C0;&#x9A8C;,hadoop11&#x3001;hadoop12&#x3001;hadoop13 &#x4E0A;&#x591A;&#x4E86; JournalNode &#x8FDB;&#x7A0B;</h1>
6 &#x683C;&#x5F0F;&#x5316; HDFS
&#x5728; hadoop11 &#x4E0A;/usr/app/hadoop-2.7.3/bin &#x76EE;&#x5F55;&#x4E0B;&#x6267;&#x884C;&#x547D;&#x4EE4;:
./hdfs namenode -format
&#x683C;&#x5F0F;&#x5316;&#x540E;&#x4F1A;&#x5728;&#x6839;&#x636E; core-site.xml &#x4E2D;&#x7684; hadoop.tmp.dir &#x914D;&#x7F6E;&#x751F;&#x6210;&#x4E2A;&#x6587;&#x4EF6;,&#x8FD9;&#x91CC;&#x6211;&#x914D;&#x7F6E;&#x7684;&#x662F;
/usr/app/hadoop-2.7.3/tmp,
&#x7136;&#x540E;&#x5C06;/hadoop-2.7.3/tmp &#x62F7;&#x8D1D;&#x5230; hadoop12 &#x7684;/hadoop-2.7.3/&#x4E0B;&#x3002;
scp -r /usr/app/hadoop-2.7.3/tmp/ root@hadoop12:/usr/app/hadoop-2.7.3/<h2 id="&#x4E5F;&#x53EF;&#x4EE5;&#x8FD9;&#x6837;&#x5728;-hadoop12-&#x4E0A;&#x6267;&#x884C;-hdfs-namenode--bootstrap-standby7-&#x683C;&#x5F0F;&#x5316;-zkfc&#x5728;-hadoop11-&#x4E0A;&#x6267;&#x884C;&#x5373;&#x53EF;">&#x4E5F;&#x53EF;&#x4EE5;&#x8FD9;&#x6837;,&#x5728; hadoop12 &#x4E0A;&#x6267;&#x884C; hdfs namenode -bootstrap Standby7 &#x683C;&#x5F0F;&#x5316; ZKFC(&#x5728; hadoop11 &#x4E0A;&#x6267;&#x884C;&#x5373;&#x53EF;)</h2>
hdfs
zkfc
-formatZK
8 &#x542F;&#x52A8; HDFS(&#x5728; hadoop11 &#x4E0A;&#x6267;&#x884C;)
sbin/start-dfs.sh
[root@hadoop11 sbin]# jps
2960 QuorumPeerMain
3698 NameNode
4116 DFSZKFailoverController
3828 DataNode
4215 Jps
3287 JournalNode
<a href="http://hadoop11:50070" target="_blank">http://hadoop11:50070</a>
<a href="http://hadoop12:500709" target="_blank">http://hadoop12:500709</a> &#x542F;&#x52A8; YARN(#####&#x6CE8;&#x610F;#####:&#x662F;&#x5728; hadoop11 &#x4E0A;&#x6267;&#x884C; start-yarn.sh,&#x628A; namenode &#x548C;
resourcemanager &#x5206;&#x5F00;&#x662F;&#x56E0;&#x4E3A;&#x6027;&#x80FD;&#x95EE;&#x9898;,&#x56E0;&#x4E3A;&#x4ED6;&#x4EEC;&#x90FD;&#x8981;&#x5360;&#x7528;&#x5927;&#x91CF;&#x8D44;&#x6E90;,&#x6240;&#x4EE5;&#x628A;&#x4ED6;&#x4EEC;&#x5206;&#x5F00;&#x4E86;,
&#x4ED6;&#x4EEC;&#x5206;&#x5F00;&#x4E86;&#x5C31;&#x8981;&#x5206;&#x522B;&#x5728;&#x4E0D;&#x540C;&#x7684;&#x673A;&#x5668;&#x4E0A;&#x542F;&#x52A8;)
sbin/start-yarn.sh
<a href="http://hadoop11:8088/cluster" target="_blank">http://hadoop11:8088/cluster</a>
10 &#x9A8C;&#x8BC1;
&#x9A8C;&#x8BC1; HDFS HA
&#x9996;&#x5148;&#x5411; hdfs &#x4E0A;&#x4F20;&#x4E00;&#x4E2A;&#x6587;&#x4EF6;
hadoop fs -put /etc/profile /
hadoop fs -ls /
&#x7136;&#x540E;&#x518D; kill &#x6389; active &#x7684; NameNodekill -9 <pid of="" nn="">
&#x901A;&#x8FC7;&#x6D4F;&#x89C8;&#x5668;&#x8BBF;&#x95EE;:<a href="http://192.168.200.12:50070" target="_blank">http://192.168.200.12:50070</a>
NameNode
hadoop12:9000&apos; (active)
&#x8FD9;&#x4E2A;&#x65F6;&#x5019; hadoop12 &#x4E0A;&#x7684; NameNode &#x53D8;&#x6210;&#x4E86; active
&#x5728;&#x6267;&#x884C;&#x547D;&#x4EE4;:
hadoop fs -ls /
-rw-r--r-- 3 root supergroup
2198 2017-07-22 01:25 /profile
&#x521A;&#x624D;&#x4E0A;&#x4F20;&#x7684;&#x6587;&#x4EF6;&#x4F9D;&#x7136;&#x5B58;&#x5728;!
!
!
&#x624B;&#x52A8;&#x542F;&#x52A8;&#x90A3;&#x4E2A;&#x6302;&#x6389;&#x7684; NameNode
sbin/hadoop-daemon.sh start namenode
&#x901A;&#x8FC7;&#x6D4F;&#x89C8;&#x5668;&#x8BBF;&#x95EE;:<a href="http://192.168.200.11:50070" target="_blank">http://192.168.200.11:50070</a>
hadoop12:9000&apos; (standby)
&#x9A8C;&#x8BC1; YARN:
&#x8FD0;&#x884C;&#x4E00;&#x4E0B; hadoop &#x63D0;&#x4F9B;&#x7684; demo &#x4E2D;&#x7684; WordCount &#x7A0B;&#x5E8F;:
hadoop jar /usr/app/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /profile /out
OK,&#x5927;&#x529F;&#x544A;&#x6210;!
!
!
&#x67E5;&#x770B; hdfs:<a href="http://hadoop11:50070/" target="_blank">http://hadoop11:50070/</a>
&#x67E5;&#x770B; RM:<a href="http://hadoop11:8088/" target="_blank">http://hadoop11:8088/</a>
&#x516D;&#x3001; &#x4E3B;&#x8282;&#x70B9;&#x5B89;&#x88C5; hbase
1.&#x4E0A;&#x4F20; hbase &#x5B89;&#x88C5;&#x5305;
hbase-1.2.6-bin.tar.gz
2.&#x89E3;&#x538B;
tar &#x2013;zxvf hbase-1.2.6-bin.tar.gz
3.&#x914D;&#x7F6E; hbase &#x96C6;&#x7FA4;
3.1 &#x8981;&#x4FEE;&#x6539; 3 &#x4E2A;&#x6587;&#x4EF6;(&#x9996;&#x5148; zk &#x96C6;&#x7FA4;&#x5DF2;&#x7ECF;&#x5B89;&#x88C5;&#x597D;&#x4E86; HMASTER REGIONSERVER)
&#x6CE8;&#x610F;:&#x8981;&#x628A; hadoop &#x7684; hdfs-site.xml &#x548C; core-site.xml &#x653E;&#x5230; hbase/conf &#x4E0B;
3.2 &#x4FEE;&#x6539; &#x73AF;&#x5883;&#x53D8;&#x91CF;
Vi /etc/profileexport JAVA_HOME=/usr/app/jdk1.8.0_77
export HADOOP_HOME=/usr/app/hadoop-2.7.3
export HBASE_HOME=/usr/app/hbase-1.2.6
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin
&#x6CE8;&#x610F;:source /etc/profile (&#x5237;&#x65B0;&#x73AF;&#x5883;&#x53D8;&#x91CF;&#x914D;&#x7F6E;&#x6587;&#x4EF6;)
3.3 &#x4FEE;&#x6539; hbase-env.sh
/usr/app/hbase-1.2.6/conf/
export JAVA_HOME= /usr/app/jdk1.8.0_77
//&#x544A;&#x8BC9; hbase &#x4F7F;&#x7528;&#x5916;&#x90E8;&#x7684; zk
export HBASE_MANAGES_ZK=false
3.4 &#x4FEE;&#x6539; hbase-site.xml &#x914D;&#x7F6E;
vim hbase-site.xml
<configuration><!-- 指定 hbase 在 HDFS 上存储的路径 -->
<property>
<name>hbase.rootdir</name>
<value>hdfs://ns1/hbase</value>
</property><!-- 指定 hbase 是分布式的 -->
<property>
<name>hbase.cluster.distributed</name>
<value>true</value>
<property>
<name>hbase.master.info.port</name>
<value>60010</value>
</property>
&lt;/property&gt;<!-- 指定 zk 的地址,多个用“,”分割 -->
<property>
<name>hbase.zookeeper.quorum</name>
<value>hadoop11:2181,hadoop12:2181,hadoop13:2181</value>
</property>
&lt;/configuration&gt;
4&#x3001;&#x589E;&#x52A0; slave &#x7684;&#x96C6;&#x7FA4;
Vim /usr/app/hbase-1.2.6/conf/regionservers
(&#x90E8;&#x7F72;&#x5230; datanode &#x4E0A;&#x9762;,&#x90A3;&#x4E00;&#x53F0;&#x542F;&#x52A8; hbase &#x90A3;&#x4E00;&#x53F0;&#x5C31;&#x662F; master)
hadoop11
hadoop12
hadoop135&#x3001;&#x62F7;&#x8D1D; hbase &#x5230;&#x5176;&#x4ED6;&#x8282;&#x70B9;
scp -r /usr/app/hbase-1.2.6
scp -r /usr/app/hbase-1.2.6
root@hadoop12:/usr/app/
root@hadoop13:/usr/app/
6&#x3001;&#x5C06;&#x914D;&#x7F6E;&#x597D;&#x7684; HBase &#x62F7;&#x8D1D;&#x5230;&#x6BCF;&#x4E00;&#x4E2A;&#x8282;&#x70B9;&#x5E76;&#x540C;&#x6B65;&#x65F6;&#x95F4;&#x3002;
7&#x3001;&#x542F;&#x52A8;&#x6240;&#x6709;&#x7684; hbase
&#x524D;&#x63D0;&#x9700;&#x8981;:Zookeeper &#x548C; Hdfs &#x542F;&#x52A8;
&#x5206;&#x522B;&#x542F;&#x52A8; zk
./zkServer.sh start
&#x542F;&#x52A8; hbase &#x96C6;&#x7FA4;
start-dfs.sh
&#x542F;&#x52A8; hbase,&#x5728;&#x4E3B;&#x8282;&#x70B9;&#x4E0A;&#x8FD0;&#x884C;:
/usr/app/hbase-1.2.6/bin/start-hbase.sh
8&#x3001;&#x901A;&#x8FC7;&#x6D4F;&#x89C8;&#x5668;&#x8BBF;&#x95EE; hbase &#x7BA1;&#x7406;&#x9875;&#x9762;
<a href="http://192.168.200.11:16010/master-status" target="_blank">http://192.168.200.11:16010/master-status</a>
9&#x3001;&#x4E3A;&#x4FDD;&#x8BC1;&#x96C6;&#x7FA4;&#x7684;&#x53EF;&#x9760;&#x6027;,&#x8981;&#x542F;&#x52A8;&#x591A;&#x4E2A; HMaster
hbase-daemon.sh start master
&#x4E03;&#x3001; &#x4E3B;&#x8282;&#x70B9;&#x5B89;&#x88C5; MySql
&#x8D26;&#x53F7;:root
&#x5BC6;&#x7801;:123456
&#x5B89;&#x88C5; mysql &#x670D;&#x52A1;&#x5668;&#x547D;&#x4EE4;&#x5982;&#x4E0B;:
yum install mysql-server
&#x8BBE;&#x7F6E;&#x5F00;&#x673A;&#x542F;&#x52A8;&#x547D;&#x4EE4;&#x5982;&#x4E0B;:chkconfig mysqld on
&#x542F;&#x52A8; mysql &#x670D;&#x52A1;&#x547D;&#x4EE4;&#x5982;&#x4E0B;:
service mysqld start
&#x5E76;&#x6839;&#x636E;&#x63D0;&#x793A;&#x8BBE;&#x7F6E; root &#x7684;&#x521D;&#x8BD5;&#x5BC6;&#x7801;&#x547D;&#x4EE4;&#x5982;&#x4E0B;:
mysqladmin -u root password 123456
&#x8FDB;&#x5165; mysql &#x547D;&#x4EE4;&#x884C;&#x547D;&#x4EE4;&#x5982;&#x4E0B;:
mysql -uroot &#x2013;p123456
&#x5728; Mysql &#x4E2D;&#x6267;&#x884C;&#x8005;&#x56DB;&#x6B65;:
create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
grant all privileges on <em>.</em> to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;
flush privileges;
&#x5907;&#x6CE8;&#x8BF4;&#x660E;:
&#x6CE8;&#x610F;:&#x62A5;&#x9519; ERROR 1130: Host &apos;192.168.200.1&apos; is not allowed to connect to thisMySQL server
&#x8FD9;&#x4E2A;&#x9519;&#x8BEF;&#x9700;&#x8981;&#x7528;&#x6237;&#x6388;&#x6743;
&#x8FD9;&#x4E24;&#x4E2A;&#x6570;&#x636E;&#x5E93;&#x4E0D;&#x77E5;&#x9053;&#x5565;&#x7528;???
&#x521B;&#x5EFA;&#x4EE5;&#x4E0B;&#x6570;&#x636E;&#x5E93;:<h1 id="hive">hive</h1>
create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<h1 id="activity-monitor">activity monitor</h1>
create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
&#x8BBE;&#x7F6E; root &#x6388;&#x6743;&#x8BBF;&#x95EE;&#x4EE5;&#x4E0A;&#x6240;&#x6709;&#x7684;&#x6570;&#x636E;&#x5E93;:<h1 id="&#x6388;&#x6743;-root-&#x7528;&#x6237;&#x5728;&#x4E3B;&#x8282;&#x70B9;&#x62E5;&#x6709;&#x6240;&#x6709;&#x6570;&#x636E;&#x5E93;&#x7684;&#x8BBF;&#x95EE;&#x6743;&#x9650;">&#x6388;&#x6743; root &#x7528;&#x6237;&#x5728;&#x4E3B;&#x8282;&#x70B9;&#x62E5;&#x6709;&#x6240;&#x6709;&#x6570;&#x636E;&#x5E93;&#x7684;&#x8BBF;&#x95EE;&#x6743;&#x9650;</h1>
grant all privileges on <em>.</em> to &apos;root&apos;@&apos;n1&apos; identified by &apos;xxxx&apos; with grant option;flush privileges;
grant all privileges on <em>.</em> to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;flush privileges;&#x5BF9;&#x7528;&#x6237;&#x6388;&#x6743;
mysql&gt;grant rights on database.<em> to user@host identified by &quot;pass&quot;;
&#x4F8B; 1:
&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x7528;&#x6237; test1 &#x5BC6;&#x7801;&#x4E3A; abc,&#x8BA9;&#x4ED6;&#x53EF;&#x4EE5;&#x5728;&#x4EFB;&#x4F55;&#x4E3B;&#x673A;&#x4E0A;&#x767B;&#x5F55;,&#x5E76;&#x5BF9;&#x6240;&#x6709;&#x6570;&#x636E;&#x5E93;&#x6709;&#x67E5;&#x8BE2;&#x3001;&#x63D2;
&#x5165;&#x3001;&#x4FEE;&#x6539;&#x3001;&#x5220;&#x9664;&#x7684;&#x6743;&#x9650;&#x3002;
grantselect,insert,update,delete on </em>.<em> to test1@&quot;%&quot; Identified by &quot;abc&quot;;
ON &#x5B50;&#x53E5;&#x4E2D;</em>.<em> &#x8BF4;&#x660E;&#x7B26;&#x7684;&#x610F;&#x601D;&#x662F;&#x201C;&#x6240;&#x6709;&#x6570;&#x636E;&#x5E93;,&#x6240;&#x6709;&#x7684;&#x8868;&#x201D;
&#x4F8B; 2:
&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x7528;&#x6237; test2 &#x5BC6;&#x7801;&#x4E3A; abc, &#x8BA9;&#x4ED6;&#x53EA;&#x53EF;&#x4EE5;&#x5728; localhost &#x4E0A;&#x767B;&#x5F55;,&#x5E76;&#x53EF;&#x4EE5;&#x5BF9;&#x6570;&#x636E;&#x5E93; mydb &#x8FDB;&#x884C;
&#x67E5;&#x8BE2;&#x3001;&#x63D2;&#x5165;&#x3001;&#x4FEE;&#x6539;&#x3001;&#x5220;&#x9664;&#x7684;&#x64CD;&#x4F5C;&#x3002;
grant select,insert,update,delete on mydb.</em> to test2@localhost identified by &quot;abc&quot;;
&#x516B;&#x3001; &#x4E3B;&#x8282;&#x70B9;&#x5B89;&#x88C5; hive &#x548C;&#x542F;&#x52A8;
hive &#x5B89;&#x88C5;&#x624B;&#x518C;:
1&#x3001;&#x4E0A;&#x4F20;&#x538B;&#x7F29;&#x5305;,&#x89E3;&#x538B;
2&#x3001;&#x5B89;&#x88C5; mysql &#x670D;&#x52A1;&#x5668;
3&#x3001;&#x8FDB;&#x5165; hive &#x7684; conf &#x76EE;&#x5F55;&#x65B0;&#x5EFA;&#x4E00;&#x4E2A; hive-site.xml
4&#x3001;&#x5728; hive-site.xml &#x4E2D;&#x5199;&#x5165; mysql &#x8FDE;&#x63A5;&#x4FE1;&#x606F;
5&#x3001;&#x5C06; mysql &#x7684;&#x9A71;&#x52A8;&#x5305;&#x590D;&#x5236;&#x5230; hive &#x7684; lib &#x76EE;&#x5F55;&#x4E0B; app/hive-0.12.0/lib
6.&#x3001; Sh /usr/app/hive-0.12.0/bin/hive &#x542F;&#x52A8; hive
Vi /etc/profile
export JAVA_HOME=/usr/app/jdk1.8.0_77
export HADOOP_HOME=/usr/app/hadoop-2.7.3
export HBASE_HOME=/usr/app/hbase-1.2.6
export HIVE_HOME=/usr/app/hive-0.12.0
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$HIVE_HOME/bin
&#x4FEE;&#x6539;/usr/app/hive-0.12.0/conf/hive-env.sh
&#x7684;&#x5C3E;&#x90E8; hive-env.sh,&#x589E;&#x52A0;&#x4EE5;&#x4E0B;&#x4E09;&#x884C;
export JAVA_HOME=/usr/app/jdk1.8.0_77
export HADOOP_HOME=/usr/app/hadoop-2.7.3
export HBASE_HOME=/usr/app/hbase-1.2.6hive-default.xml.template
&#x6539;&#x4E3A; hive-site.xml
&#x4FEE;&#x6539;/usr/app/hive-0.12.0/conf/hive-site.xml
/usr/app/hive-site.xml
hive-site.xml &#x4E3B;&#x8981;&#x914D;&#x7F6E;
hive-site.xml
<configuration>
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://hadoop11:3306/hive?createDatabaseIfNotExist=true</value>
<description>JDBC connect string for a JDBC metastore</description>
</property>
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
<description>Driver class name for a JDBC metastore</description>
</property>
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
<description>username to use against metastore database</description>
</property>
<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>123456</value>
<description>password to use against metastore database</description>
</property>
<property>
<name>hive.server2.thrift.sasl.qop</name>
<value>auth</value>
</property>
<property>
<name>hive.metastore.schema.verification</name>
<value>false</value>
</property>
</configuration>&#x9A8C;&#x8BC1; hive &#x5B89;&#x88C5;
Sh /usr/app/hive-0.12.0/bin/hive
hive&gt; create table test(id int,name string);
OK
Time taken: 8.292 seconds
hive&gt; show tables;
OK
test
[root@hadoop13 ~]# hadoop fs -lsr /
drwxr-xr-x - root supergroup
drwxr-xr-x - root supergroup
drwxr-xr-x - root supergroup
drwxr-xr-x - root supergroup
0 2016-01-10 20:57 /user
0 2016-01-10 20:57 /user/hive
0 2016-01-11 01:46 /user/hive/warehouse
0 2016-01-11 01:46 /user/hive/warehouse/test
&#x4E5D;&#x3001; Apache &#x642D;&#x5EFA; Flume</property></configuration></pid></li>
<li>Flume &#x4E0B;&#x8F7D;&#x5730;&#x5740;
apache-flume-1.6.0-bin.tar.gz
<a href="http://pan.baidu.com/s/1o81nR8e" target="_blank">http://pan.baidu.com/s/1o81nR8e</a> s832
&#x5B98;&#x7F51;
<a href="https://flume.apache.org/download.html" target="_blank">https://flume.apache.org/download.html</a></li>
<li>&#x5B89;&#x88C5;
[root@hadoop11 ~]# cd /usr/app/
[root@hadoop11 app]# tar -zxvf apache-flume-1.6.0-bin.tar.gz[root@hadoop11 app]# mv apache-flume-1.6.0-bin flume1.6</li>
<li>&#x914D;&#x7F6E;
1) /etc/profile
[root@hadoop11 app]# vi /etc/profile
[root@hadoop11 app]# source /etc/profile
export JAVA_HOME=/usr/app/jdk1.8.0_77
export HADOOP_HOME=/usr/app/hadoop-2.7.3
export HBASE_HOME=/usr/app/hbase-1.2.6
export HIVE_HOME=/usr/app/hive-0.12.0
export Flume_HOME=/usr/app/flume1.6
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$HIVE_HOME/bin:$Flume_HOME/bin
2) /usr/app/flume1.6/conf/flume-env.sh
[root@hadoop11 conf]# cp -r flume-env.sh.template flume-env.sh
[root@hadoop11 conf]# chmod 777 flume-env.sh
[root@hadoop11 conf]# vi flume-env.sh</li>
<li>&#x6DFB;&#x52A0; Java &#x8DEF;&#x5F84;
&#x8BBE;&#x7F6E;&#x6743;&#x9650;[root@hadoop11 conf]# chmod 777 flume-env.sh</li>
<li>&#x6D4B;&#x8BD5;&#x914D;&#x7F6E;
&#x5341;&#x3001; Kafka &#x5B89;&#x88C5;&#x548C;&#x4F7F;&#x7528;</li>
<li>&#x4E0B;&#x8F7D;&#x89E3;&#x538B;
kafka_2.10-0.8.1.1.tgz
[root@hadoop11 app]# tar &#x2013;zxvf kafka_2.10-0.8.1.1.tgz
[root@hadoop11 app]# mv kafka_2.10-0.8.1.1 kafka2.10
[root@hadoop11 kafka2.10]# pwd
/usr/app/kafka2.10</li>
<li>&#x5B89;&#x88C5;&#x914D;&#x7F6E;
&#x89E3;&#x538B;&#x8FDB;&#x5165; kafka &#x76EE;&#x5F55;
[root@hadoop11 kafka2.10]# vi /usr/app/kafka2.10/config/server.properties
&#x914D;&#x7F6E;&#x6587;&#x4EF6;server.properties
.docx<h1 id="hostname-the-broker-will-bind-to-if-not-set-the-server-will-bind-to-all">Hostname the broker will bind to. If not set, the server will bind to all</h1>
interfaces
host.name=hadoop11
&#x8FD9;&#x91CC;&#x914D;&#x7F6E;&#x672C;&#x673A;&#x540D;(&#x4E0E;&#x4E3B;&#x673A;&#x540D;&#x79F0;&#x4E00;&#x6837;)
&#x4F20;&#x5230;&#x5176;&#x4F59;&#x673A;&#x5668;
[root@hadoop11 kafka2.10]#scp -r /usr/app/kafka2.10 root@hadoop12:/usr/app
[root@hadoop11 kafka2.10]#scp -r /usr/app/kafka2.10 root@hadoop13:/usr/app</li>
<li>&#x542F;&#x52A8;
&#x542F;&#x52A8; kafka &#x4E4B;&#x524D;,&#x9700;&#x8981;&#x542F;&#x52A8; Zookeeper
&#x5728;&#x88C5; kafka &#x7684;&#x673A;&#x5668;&#x4E0A;&#x542F;&#x52A8;(hadoop11,hadoop12,hadoop13)
[root@hadoop11 kafka2.10]# /usr/app/kafka2.10/bin/kafka-server-start.sh /usr/app/kafka2.10/config/server.properties&amp;4. &#x6D4B;&#x8BD5;
Kafka &#x64CD;&#x4F5C;&#x8DEF;&#x5F84;
/usr/app/kafka2.10/bin<h1 id="&#x5728;-hadoop11-&#x4E0A;&#x9762;&#x521B;&#x5EFA;-kafka-&#x7684;-topic">&#x5728; hadoop11 &#x4E0A;&#x9762;&#x521B;&#x5EFA; kafka &#x7684; topic</h1>
./kafka-topics.sh &#x2013;create.sh --topic orcale --replication-factor 1 --partitions 2 --zookeeper hadoop11:2181<h1 id="kafka-&#x751F;&#x4EA7;&#x8005;&#x547D;&#x4EE4;">kafka &#x751F;&#x4EA7;&#x8005;&#x547D;&#x4EE4;</h1>
./kafka-console-producer.sh --broker-list hadoop11:9092 --sync --topic orcale#kafka &#x6D88;&#x8D39;&#x8005;&#x547D;&#x4EE4;
./kafka-console-consumer.sh --zookeeper hadoop11:2181 --topic orcale --from-beginning<h1 id="kafka-&#x770B;-topic-&#x5F97;-list">kafka &#x770B; topic &#x5F97; list</h1>
[root@hadoop11 kafka2.10]#
./kafka-topics.sh --list --zookeeper localhost:2181<h1 id="kafka-&#x5220;&#x9664;-topic">kafka &#x5220;&#x9664; topic</h1>
./kafka-topics.sh --zookeeper localhost:2181 --topic oracle --delete<h1 id="kafka-&#x5173;&#x95ED;&#x8FDB;&#x7A0B;">kafka &#x5173;&#x95ED;&#x8FDB;&#x7A0B;</h1>
/usr/app/kafka2.10/bin/kafka-server-stop.sh
&#x5341;&#x4E00;&#x3001; Scala &#x5B89;&#x88C5;
&#x4E0B;&#x8F7D;&#x5730;&#x5740; :
<a href="http://www.scala-lang.org/download/2.11.11.html" target="_blank">http://www.scala-lang.org/download/2.11.11.html</a>
[root@hadoop11 ~]# cd /usr/app/
[root@hadoop11 app]# tar -zxvf scala-2.11.11.tgz
&#x914D;&#x7F6E;&#x73AF;&#x5883;
export JAVA_HOME=/usr/app/jdk1.8.0_77export HADOOP_HOME=/usr/app/hadoop-2.7.3
export HBASE_HOME=/usr/app/hbase-1.2.6
export HIVE_HOME=/usr/app/hive-0.12.0
export Flume_HOME=/usr/app/flume1.6
export Scala_HOME=scala-2.11.11export
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$HIVE_HOME/bin:$Flume_HOME/bin:
$Scala_HOME/bin
&#x5237;&#x65B0;&#x914D;&#x7F6E;&#x6587;&#x4EF6;,&#x68C0;&#x6D4B; scala &#x7248;&#x672C;
[root@hadoop11 app]# source /etc/profile
[root@hadoop11 app]# scala -version
Scala code runner version 2.11.11 -- Copyright 2002-2017, LAMP/EPFL
&#x68C0;&#x6D4B; scala &#x7684;&#x5BA2;&#x6237;&#x7AEF;
[root@hadoop11 app]# scala
Welcome to Scala 2.11.11 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_77).
Type in expressions for evaluation. Or try :help.
scala&gt; print(&quot;Hello Scala&quot;)
Hello Scala
&#x5341;&#x4E8C;&#x3001; Spakr &#x5B89;&#x88C5;
&#x4E0B;&#x8F7D;&#x5730;&#x5740; : <a href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>
&#x4E0B;&#x8F7D;&#x89E3;&#x538B;
[root@hadoop11 ~]# cd /usr/app/
[root@hadoop11 app]# tar -zxvf spark-2.0.0-bin-hadoop2.7
&#x5411;&#x73AF;&#x5883;&#x53D8;&#x91CF;&#x6DFB;&#x52A0; spark home
[root@hadoop11 app]# vi /etc/profile
export JAVA_HOME=/usr/app/jdk1.8.0_77
export HADOOP_HOME=/usr/app/hadoop-2.7.3
export HBASE_HOME=/usr/app/hbase-1.2.6
export HIVE_HOME=/usr/app/hive-0.12.0
export Flume_HOME=/usr/app/flume1.6
export Spark_HOME=/usr/app/spark-2.0.0-bin-hadoop2.7
export Scala_HOME=scala-2.11.11export
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$HIVE_HOME/bin:$Fl
ume_HOME/bin:$Spark_HOME/bin:$Scala_HOME/bin
&#x914D;&#x7F6E;./conf/slaves
&#x9996;&#x5148;&#x5C06; slaves.template &#x62F7;&#x8D1D;&#x4E00;&#x4EFD;
[root@hadoop11 conf]# cp -r slaves.template slaves
&#x4FEE;&#x6539; slaves &#x6587;&#x4EF6;
hadoop11
hadoop12
hadoop13
&#x914D;&#x7F6E;./conf/spark-env.sh
&#x540C;&#x6837;&#x5C06; spark-env.sh. template &#x62F7;&#x8D1D;&#x4E00;&#x4EFD;
[root@hadoop11 conf]# cp -r spark-env.sh.template spark-env.sh
&#x4FEE;&#x6539; /conf/spark-env.sh
export JAVA_HOME=/usr/app/jdk1.8.0_77
export Scala_HOME=scala-2.11.11
export SPARK_MASTER_IP=hadoop11
export SPARK_WORKER_MEMORY=2g
export MASTER=spark://hadoop11:7077
&#x6700;&#x540E;&#x5C06; spark-1.6.1-bin-hadoop2.6 &#x6587;&#x4EF6;&#x5939;&#x62F7;&#x8D1D;&#x5230;&#x53E6;&#x5916;&#x4E24;&#x4E2A;&#x7ED3;&#x70B9;&#x5373;&#x53EF;&#x3002;
&#x5176;&#x4F59;&#x4E24;&#x53F0;&#x7684;&#x73AF;&#x5883;&#x53D8;&#x91CF;
export JAVA_HOME=/usr/app/jdk1.8.0_77
export HADOOP_HOME=/usr/app/hadoop-2.7.3
export HBASE_HOME=/usr/app/hbase-1.2.6
export Spark_HOME=/usr/app/spark-2.0.0-bin-hadoop2.7
export Scala_HOME=scala-2.11.11
export
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$Spark_HOME/bin:$S
cala_HOME/bin&#x542F;&#x52A8;
[root@hadoop11 spark-2.0.0-bin-hadoop2.7]# ./sbin/start-all.sh
[root@hadoop11 bin]# spark-shell&#x5341;&#x4E09;&#x3001; &#x542F;&#x52A8;&#x987A;&#x5E8F;&#x53CA;&#x8FDB;&#x7A0B;&#x89E3;&#x8BF4;
1) &#x8FDB;&#x7A0B;&#x89E3;&#x8BF4;
DataNodeHadoop &#x653E;&#x5728; slaves &#x4E2D;
NameNodeHadoop &#x5206;&#x914D;&#x4EFB;&#x52A1;
HRegionServerhbase &#x4ECE;
QuorumPeerMainzookeeper
HMasterhbase &#x4E3B;
JournalNode &#x6307;&#x5B9A; NameNode &#x7684; edits &#x5143;&#x6570;&#x636E;&#x5728; JournalNode &#x4E0A;&#x7684;&#x5B58;&#x653E;&#x4F4D;&#x7F6E;
DFSZKFailoverControllerzkfcnamenode &#x7684;&#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
ResourceManager
yarn &#x91CC;&#x9762;&#x7684;&#x8001;&#x5927;
Nodemanageryarn &#x7684;&#x968F;&#x4ECE;
2) &#x542F;&#x52A8;&#x987A;&#x5E8F;
/usr/app/zookeeper-3.4.5/bin/./zkServer.sh start(&#x6BCF;&#x53F0;)
/usr/app/hadoop-2.6.0/sbin/start-dfs.sh(hadoop11)
/usr/app/hadoop-2.6.0/sbin/start-yarn.sh(hadoop11)
/usr/app/hbase/bin/start-hbase.sh(hadoop11)
/usr/app/hbase/bin/hbase-daemon.sh start master(hadoop12)
/usr/app/kafka2.10/bin/kafka-server-start.sh /usr/app/kafka2.10/config/server.properties&amp; (&#x6BCF;&#x53F0; kafka)
3) &#x5173;&#x95ED;&#x987A;&#x5E8F;
/usr/app/kafka2.10/bin/kafka-server-stop.sh(&#x6BCF;&#x53F0; kafka)
/usr/app/hbase/bin/hbase-daemon.sh stop master(hadoop12)
/usr/app/hbase/bin/stop-hbase.sh(hadoop11)
/usr/app/hadoop-2.7.3/sbin/stop-yarn.sh(hadoop11)
/usr/app/hadoop-2.7.3/sbin/stop-dfs.sh(hadoop11)
/usr/app/zookeeper-3.4.8/bin/./zkServer.sh stop(&#x6BCF;&#x53F0;)4) &#x67E5;&#x770B;
<a href="http://192.168.200.11:50070" target="_blank">http://192.168.200.11:50070</a> (HDFS &#x7BA1;&#x7406;&#x754C;&#x9762;)(active)
<a href="http://192.168.200.12:50070" target="_blank">http://192.168.200.12:50070</a> (HDFS &#x7BA1;&#x7406;&#x754C;&#x9762;)(standby)
<a href="http://192.168.200.11:8088(yarn" target="_blank">http://192.168.200.11:8088(yarn</a> &#x7BA1;&#x7406;&#x754C;&#x9762;)&#x542F;&#x52A8; yarn &#x624D;&#x80FD;&#x542F;&#x52A8;
<a href="http://192.168.200.11:60010(hbase" target="_blank">http://192.168.200.11:60010(hbase</a> &#x7BA1;&#x7406;&#x9875;&#x9762;)
<a href="http://192.168.200.12:60010(hbase" target="_blank">http://192.168.200.12:60010(hbase</a> &#x7BA1;&#x7406;&#x9875;&#x9762;&#x526F;&#x672C;)
5) hadoop &#x542F;&#x52A8;&#x6307;&#x4EE4;
start-all.sh &#x542F;&#x52A8;&#x6240;&#x6709;&#x7684; Hadoop &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;&#x3002;&#x5305;&#x62EC; NameNode&#x3001; Secondary NameNode&#x3001;DataNode&#x3001;JobTracker&#x3001;TaskTrack
stop-all.sh &#x505C;&#x6B62;&#x6240;&#x6709;&#x7684; Hadoop &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;&#x3002;&#x5305;&#x62EC; NameNode&#x3001; Secondary NameNode&#x3001;DataNode&#x3001;JobTracker&#x3001;TaskTrack
start-dfs.sh &#x542F;&#x52A8; Hadoop HDFS &#x5B88;&#x62A4;&#x8FDB;&#x7A0B; NameNode&#x3001;SecondaryNameNode &#x548C; DataNode
stop-dfs.sh &#x505C;&#x6B62; Hadoop HDFS &#x5B88;&#x62A4;&#x8FDB;&#x7A0B; NameNode&#x3001;SecondaryNameNode &#x548C; DataNode
hadoop-daemons.sh start namenode &#x5355;&#x72EC;&#x542F;&#x52A8; NameNode &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
hadoop-daemons.sh stop namenode &#x5355;&#x72EC;&#x505C;&#x6B62; NameNode &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
hadoop-daemons.sh start datanode &#x5355;&#x72EC;&#x542F;&#x52A8; DataNode &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
hadoop-daemons.sh stop datanode &#x5355;&#x72EC;&#x505C;&#x6B62; DataNode &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
hadoop-daemons.sh start secondarynamenode &#x5355;&#x72EC;&#x542F;&#x52A8; SecondaryNameNode &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
hadoop-daemons.sh stop secondarynamenode &#x5355;&#x72EC;&#x505C;&#x6B62; SecondaryNameNode &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
start-mapred.sh &#x542F;&#x52A8; Hadoop MapReduce &#x5B88;&#x62A4;&#x8FDB;&#x7A0B; JobTracker &#x548C; TaskTracker
stop-mapred.sh &#x505C;&#x6B62; Hadoop MapReduce &#x5B88;&#x62A4;&#x8FDB;&#x7A0B; JobTracker &#x548C; TaskTracker
hadoop-daemons.sh start jobtracker &#x5355;&#x72EC;&#x542F;&#x52A8; JobTracker &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
hadoop-daemons.sh stop jobtracker &#x5355;&#x72EC;&#x505C;&#x6B62; JobTracker &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
hadoop-daemons.sh start tasktracker &#x5355;&#x72EC;&#x542F;&#x52A8; TaskTracker &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
hadoop-daemons.sh stop tasktracker &#x5355;&#x72EC;&#x542F;&#x52A8; TaskTracker &#x5B88;&#x62A4;&#x8FDB;&#x7A0B;
&#x5341;&#x56DB;&#x3001; &#x9519;&#x8BEF;&#x96C6;&#x5408;</li>
<li>&#x9519;&#x8BEF;:MysqlMysql ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos;&#x95EE;
&#x9898;&#x7684;&#x89E3;&#x51B3;</li>
<li>&#x5173;&#x95ED; mysql
service mysqld stop</li>
<li>&#x5C4F;&#x853D;&#x6743;&#x9650;
mysqld_safe --skip-grant-table</li>
<li>&#x65B0;&#x5F00;&#x8D77;&#x4E00;&#x4E2A;&#x7EC8;&#x7AEF;&#x8F93;&#x5165;
mysql -u root mysql
mysql&gt;UPDATE user SET Password=PASSWORD(&#x2018;123456&#x2019;) where
USER=&#x2018;root&#x2018;;
mysql&gt; FLUSH PRIVILEGES;
mysql&gt; \q</li>
<li>&#x9519;&#x8BEF;:Hbase
Could not locate executable null\bin\winutils.exe in the Hadoop binaries
System.setProperty(&quot;hadoop.home.dir&quot;, &quot;G:/hadoop/hadoop-2.4.1&quot;);
&#x4E0B;&#x8F7D; hadoop2.6(x64)V0.2.zip &#x653E;&#x5165; D:\Java\hadoop-2.6.0\bin &#x4E2D;</li>
<li>&#x9519;&#x8BEF;:Hbase<a href="http://blog.csdn.net/zzu09huixu/article/details/28448705" target="_blank">http://blog.csdn.net/zzu09huixu/article/details/28448705</a>
2017-03-05 02:51:23,887 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: Failed construction of Regionserver: class org.apache.hadoop.hbase.regionserver.HRegionServer
at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2487)
at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:64)
at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:87)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2502)
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2485)
... 5 more
Caused by: java.net.BindException: Problem binding to hadoop11/192.168.200.11:16020 : &#x5730;&#x5740;&#x5DF2;&#x5728;&#x4F7F;&#x7528;
at org.apache.hadoop.hbase.ipc.RpcServer.bind(RpcServer.java:2371)
at org.apache.hadoop.hbase.ipc.RpcServer$Listener.<init>(RpcServer.java:524)
at org.apache.hadoop.hbase.ipc.RpcServer.<init>(RpcServer.java:1899)
at org.apache.hadoop.hbase.regionserver.RSRpcServices.<init>(RSRpcServices.java:790)
at org.apache.hadoop.hbase.regionserver.HRegionServer.createRpcServices(HRegionServer.java:575)
at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:492)
... 10 more
Caused by: java.net.BindException: &#x5730;&#x5740;&#x5DF2;&#x5728;&#x4F7F;&#x7528;
at sun.nio.ch.Net.bind0(Native Method)
at sun.nio.ch.Net.bind(Net.java:444)
at sun.nio.ch.Net.bind(Net.java:436)
at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
at org.apache.hadoop.hbase.ipc.RpcServer.bind(RpcServer.java:2369)</init></init></init></init></li>
<li>&#x9519;&#x8BEF;:hbase
starting master, logging to /usr/app/hbase-1.2.6/bin/../logs/hbase-root-master-hadoop11.out
hadoop12:
starting
regionserver,
logging
to
/usr/app/hbase-1.2.6/bin/../logs/hbase-root-regionserver-hadoop12.out
hadoop11:
starting
regionserver,
logging
to
/usr/app/hbase-1.2.6/bin/../logs/hbase-root-regionserver-hadoop11.out
hadoop13:
starting
regionserver,
logging
to
/usr/app/hbase-1.2.6/bin/../logs/hbase-root-regionserver-hadoop13.outhadoop11: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m;
support was removed in 8.0
hadoop11: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m;
support was removed in 8.0
hadoop13: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m;
support was removed in 8.0
hadoop13: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m;
support was removed in 8.0
&#x89E3;&#x51B3;&#x65B9;&#x6848;:
&#x67E5;&#x770B;&#x4E86; zk &#x8282;&#x70B9;&#x7684;&#x8BBE;&#x7F6E;&#x53D1;&#x73B0;&#x8282;&#x70B9;&#x7684;&#x8BBE;&#x7F6E;&#x786E;&#x5B9E;&#x6709;&#x95EE;&#x9898;,&#x5728; zk &#x7684; configuration &#x5B57;
&#x7B26;&#x4E32;&#x4E2D;&#x95F4;&#x591A;&#x4E86;&#x4E2A;&#x7A7A;&#x683C;,(&#x2299;o&#x2299;)...
&#x4FEE;&#x6539;&#x4E4B;&#x540E;,&#x9519;&#x8BEF;&#x6D88;&#x5931; &#x3002;</li>
<li>&#x9519;&#x8BEF;:Hbase &#x8FDE;&#x63A5;&#x96C6;&#x7FA4;
17/04/16 16:09:14 INFO zookeeper.ClientCnxn: Session establishment complete on server
hadoop11/192.168.200.11:2181, sessionid = 0x15b762a42df0008, negotiated timeout = 40000
java.io.IOException: java.lang.reflect.InvocationTargetException
at
org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:24
0)
at
org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:
414)
at
org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:
407)
at
org.apache.hadoop.hbase.client.ConnectionManager.getConnectionInternal(ConnectionManager
.java:285)
at org.apache.hadoop.hbase.client.HBaseAdmin.<init>(HBaseAdmin.java:207)
at cn.orcale.com.bigdata.hbase.HbaseDao.createTable(HbaseDao.java:73)
at cn.orcale.com.bigdata.hbase.HbaseDao.main(HbaseDao.java:48)
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.
java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at
org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:23
8)
... 6 more
Caused
by:
java.lang.VerifyError:
class
org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Result overrides final method
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at org.apache.hadoop.hbase.protobuf.ProtobufUtil.<clinit>(ProtobufUtil.java:211)
at org.apache.hadoop.hbase.ClusterId.parseFrom(ClusterId.java:64)
at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:75)
at
org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:86)
at
org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClust
erId(ConnectionManager.java:850)
at
org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(Conne
ctionManager.java:635)
... 11 more
17/04/16 16:09:14 INFO hbase.HbaseDao: end create table ......
&#x89E3;&#x51B3;&#x65B9;&#x6CD5;:&#x5916;&#x754C;&#x7684;&#x5305;&#x548C; Maven &#x4E2D;&#x7684;&#x5305;&#x6709;&#x51B2;&#x7A81;6. &#x9519;&#x8BEF;:HDFS &#x8FDE;&#x63A5;&#x96C6;&#x7FA4;
17/04/16 16:39:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your
platform... using builtin-java classes where applicable
Exception
in
thread
&quot;main&quot;
java.lang.VerifyError:
class
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto
overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.getDeclaredMethods0(Native Method)at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
at java.lang.Class.privateGetPublicMethods(Class.java:2902)
at java.lang.Class.privateGetPublicMethods(Class.java:2911)
at java.lang.Class.getMethods(Class.java:1615)
at sun.misc.ProxyGenerator.generateClassFile(ProxyGenerator.java:451)
at sun.misc.ProxyGenerator.generateProxyClass(ProxyGenerator.java:339)
at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:639)
at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:557)
at java.lang.reflect.WeakCache$Factory.get(WeakCache.java:230)
at java.lang.reflect.WeakCache.get(WeakCache.java:127)
at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:419)
at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:719)
at org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy(ProtobufRpcEngine.java:105)
at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:570)
at
org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies
.java:420)
at
org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:316)
at
org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.getProxy(Configur
edFailoverProxyProvider.java:124)
at
org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:73)
at
org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:64)
at org.apache.hadoop.io.retry.RetryProxy.create(RetryProxy.java:58)
at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:183)
at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:664)
at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:608)
at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2596)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:169)
at cn.orcale.com.bigdata.hdfs.HdfsTest.uploadFile(HdfsTest.java:36)
at cn.orcale.com.bigdata.hdfs.HdfsTest.main(HdfsTest.java:122)
&#x89E3;&#x51B3;&#x65B9;&#x6CD5;:&#x5916;&#x754C;&#x7684;&#x5305;&#x548C; Maven &#x4E2D;&#x7684;&#x5305;&#x6709;&#x51B2;&#x7A81;7. &#x9519;&#x8BEF;:NameNode
2017-03-05 10:53:19,411 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/app/hadoop-2.6.0/tmp/dfs/name is
in an inconsistent state: storage directory does not exist or is not accessible.
at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2017-03-05 10:53:19,413 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2017-03-05 10:53:19,423 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:
/<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><em>**</em></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>SHUTDOWN_MSG: Shutting down NameNode at hadoop12/192.168.200.12
<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><em>**</em></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>/</init></init></init></init></init></init></init></clinit></init></li>
<li>&#x9519;&#x8BEF;:Hive01
<a href="http://www.cnblogs.com/simple-focus/p/6184581.html" target="_blank">http://www.cnblogs.com/simple-focus/p/6184581.html</a>
hive &gt; show databases;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.
java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
hive&gt; create table test(id int,name string);
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.
java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
1&#x3001;&#x5C06; mysql &#x7684;&#x9A71;&#x52A8;&#x5305;&#x590D;&#x5236;&#x5230; hive &#x7684; lib &#x76EE;&#x5F55;&#x4E0B; app/hive-0.12.0/lib
mysql-connector-java-5.1.36-bin.jar
2&#x3001;&#x5728; mysq &#x4E2D;&#x521B;&#x5EFA; hive &#x7684;&#x7528;&#x6237;,&#x4E14;&#x6388;&#x4E88;&#x6743;&#x9650;,&#x6267;&#x884C;&#x4E0B;&#x9762;&#x64CD;&#x4F5C;
&#x5728; Mysql &#x4E2D;&#x6267;&#x884C;&#x8005;&#x56DB;&#x6B65;:
create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
grant all privileges on <em>.</em> to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;
flush privileges;grant all privileges on <em>.</em> to &apos;root&apos;@&apos;n1&apos; identified by &apos;xxxx&apos; with grant option;flush privileges;
grant all privileges on <em>.</em> to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;flush privileges;</li>
<li>&#x9519;&#x8BEF;:Hive02
Hive&gt; CREATE TABLE dummy(value STRING);
FAILED: Execution Error, return code 1 from
org.apache.hadoop.hive.ql.exec.DDLTask.
MetaException(message:javax.jdo.JDODataStoreException: An exception
was thrown while adding/validating class(es) : Specified key was too long;
max key length is 767 bytes
com.MySQL.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified
key was too long; max key length is 767 bytes
&#x89E3;&#x51B3;&#x65B9;&#x6CD5;:
mysql &gt; alter database hive character set latin1;
&#x518D;&#x5728; Hive &#x91CC;&#x521B;&#x5EFA;&#x8868;&#x5DF2;&#x7ECF; ok &#x4E86;
10.
&#x9519;&#x8BEF;:Hive03
<a href="http://www.micmiu.com/bigdata/hive/hive-exception-not-a-host-port-parir-pbuf/" target="_blank">http://www.micmiu.com/bigdata/hive/hive-exception-not-a-host-port-parir-pbuf/</a>
&#x89E3;&#x51B3;&#x95EE;&#x9898;:<a href="http://www.aboutyun.com/thread-7881-1-1.html" target="_blank">http://www.aboutyun.com/thread-7881-1-1.html</a>
11.
&#x9519;&#x8BEF;:Hive04
hive&gt; create table test(id int,name string);
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.MetaException(message:javax.jdo.JDODataStoreException: An exception was thrown while
adding/validating class(es) : Specified key was too long; max key length is 767 bytes
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max
key length is 767 bytes
&#x89E3;&#x51B3;&#x65B9;&#x6CD5;:
mysql &gt;
alter database hive character set latin1;
&#x518D;&#x5728; Hive &#x91CC;&#x521B;&#x5EFA;&#x8868;&#x5DF2;&#x7ECF; ok &#x4E86;</li>
</ol>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../" class="navigation navigation-prev " aria-label="Previous page: 简介">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="1.html" class="navigation navigation-next " aria-label="Next page: 第一节 VMware16安装 ">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"第一章","level":"1.2","depth":1,"next":{"title":"第一节 VMware16安装 ","level":"1.2.1","depth":2,"path":"part1/1.md","ref":"part1/1.md","articles":[]},"previous":{"title":"简介","level":"1.1","depth":1,"path":"README.md","ref":"README.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"part1/README.md","mtime":"2021-05-07T07:59:35.786Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2021-05-08T02:11:46.700Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

